from __future__ import absolute_import, division, print_function, unicode_literals

import time

import shap
import tensorflow as tf
from tensorflow import keras
import numpy as np
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import csv
import os

import eli5
from eli5.sklearn import PermutationImportance

from tensorflow.python.keras.utils.np_utils import normalize, to_categorical
from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor

FOLDER='result_fin'
colums_names=[]
# запись векторов разрещений и результата
Y_vector=None
Permissions_vector=None
with open(FOLDER+'/permission_dict.csv', newline='') as File:
    reader = csv.reader(File,delimiter=';')
    for row in reader:
        if reader.line_num == 1:
            colums_names.append(row[2:])
            continue
        else:
            if Y_vector is None:
                if row[1].find('virus') != -1:
                    Y_vector=np.array([1])
                else:
                    Y_vector=np.array([0])
            else:
                if row[1].find('virus') != -1:
                    Y_vector=np.append(Y_vector,[1],axis=0)
                else:
                    Y_vector=np.append(Y_vector,[0],axis=0)
            temp = [int(i) for i in row[2:]]
            if Permissions_vector is None:
                Permissions_vector=np.array([temp])
            else:
                Permissions_vector=np.append(Permissions_vector,[temp],axis=0)



OtherDict_vector=None
with open(FOLDER+'/OtherDict.csv', newline='') as File:
    reader = csv.reader(File,delimiter=';')
    for row in reader:
        if reader.line_num == 1:
            colums_names.append(row[2:])
            continue
        else:
            temp = [int(i) for i in row[2:]]
            if OtherDict_vector is None:
                OtherDict_vector=np.array([temp])
            else:
                OtherDict_vector=np.append(OtherDict_vector,[temp],axis=0)

API_CALLS_vector=None
with open(FOLDER+'/API_CALLS.csv', newline='') as File:
    reader = csv.reader(File,delimiter=';')
    for row in reader:
        if reader.line_num == 1:
            colums_names.append(row[2:])
            continue
        else:
            temp = [int(i) for i in row[2:]]

            if API_CALLS_vector is None:
                API_CALLS_vector=np.array([temp])
            else:
                API_CALLS_vector=np.append(API_CALLS_vector,[temp],axis=0)
#API_CALLS_vector=normalize(API_CALLS_vector,axis=1)#нормализация тут- сомниельные результаты

# нормализация
temp=np.max(np.abs(API_CALLS_vector+1), axis=0)

with open('API_CALLS_normalization.npy', 'wb') as f:
    np.save(f, temp)

API_CALLS_vector=API_CALLS_vector/temp

groupAPI_dict_vector=None
with open(FOLDER+'/groupAPI_dict.csv', newline='') as File:
    reader = csv.reader(File,delimiter=';')
    for row in reader:
        if reader.line_num == 1:
            colums_names.append(row[2:])
            continue
        else:
            temp = [int(i) for i in row[2:]]

            if groupAPI_dict_vector is None:
                groupAPI_dict_vector=np.array([temp])
            else:
                groupAPI_dict_vector=np.append(groupAPI_dict_vector,[temp],axis=0)
#groupAPI_dict_vector=normalize(groupAPI_dict_vector,axis=1)

# нормализация
temp=np.max(np.abs(groupAPI_dict_vector+1), axis=0)

with open('groupAPI_dict_normalization.npy', 'wb') as f:
    np.save(f, temp)

groupAPI_dict_vector=groupAPI_dict_vector/temp


strings_dict_vector=None
with open(FOLDER+'/strings_dict.csv', newline='') as File:
    reader = csv.reader(File,delimiter=';')
    for row in reader:
        if reader.line_num is 1:
            colums_names.append(row[2:])
            continue
        else:
            temp = [int(i) for i in row[2:]]

            if strings_dict_vector is None:
                strings_dict_vector=np.array([temp])
            else:
                strings_dict_vector=np.append(strings_dict_vector,[temp],axis=0)

#попытка нормализировать
#strings_dict_vector=normalize(strings_dict_vector,axis=1)

#strings_dict_vector3=(strings_dict_vector-strings_dict_vector.mean(axis=0))/strings_dict_vector.std(axis=0)

# нормализация
temp=np.max(np.abs(strings_dict_vector+1), axis=0)

with open('string_normalization.npy', 'wb') as f:
    np.save(f, temp)

strings_dict_vector=strings_dict_vector/temp

result_vector=np.append(Permissions_vector,OtherDict_vector,axis=1)
result_vector=np.append(result_vector,API_CALLS_vector,axis=1)
result_vector=np.append(result_vector,groupAPI_dict_vector,axis=1)
result_vector=np.append(result_vector,strings_dict_vector,axis=1)



# Define per-fold score containers <-- these are new
acc_per_fold = []
loss_per_fold = []
train_acc_per_fold = []
train_loss_per_fold = []
# Define the K-fold Cross Validator
mykfold = KFold(n_splits=10, shuffle=True,random_state=6)

# чекпоинт обучения
checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
# Загрузим веса
#model.load_weights(checkpoint_path)

# Создаем коллбек сохраняющий веса модели
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1,save_freq=6)
start_time = time.time()
# K-fold Cross Validation model evaluation
fold_no = 1
model=None

# для categorical_crossentropy и Dense(2
#Y_vector=to_categorical(Y_vector)
allHistory=[]
test= None
for train, test in mykfold.split(result_vector, Y_vector):

    model = keras.Sequential([
        keras.layers.Dense(768, activation='relu', input_shape=(538,)),
        keras.layers.Dropout(0.5),
       # keras.layers.Dense(200, activation='relu'),
        #keras.layers.Dense(538*2, activation='relu'),
        keras.layers.Dense(768, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(768, activation='sigmoid'),
        #keras.layers.Dense(256, activation='sigmoid'),
        #keras.layers.Dropout(0.5),
        #keras.layers.Dense(538, activation='relu'),
        #keras.layers.Dense(50, activation='sigmoid'),
        keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    history=model.fit(result_vector[train], Y_vector[train], epochs=140,shuffle=True
                        ,use_multiprocessing=True
                        #,workers=3
                        #,callbacks=[cp_callback]
                        )
    allHistory.append(history)
    # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')
    # Generate generalization metrics
    test_loss, test_acc = model.evaluate(result_vector[test], Y_vector[test], verbose=1)
    print(
        f'Score for fold {fold_no}: {model.metrics_names[0]} of {test_loss}; {model.metrics_names[1]} of {test_acc * 100}%')
    acc_per_fold.append(test_acc * 100)
    loss_per_fold.append(test_loss)
    train_acc_per_fold.append(history.history['accuracy'][-1])
    train_loss_per_fold.append(history.history['loss'][-1])

    print(f'Test samples: {list(test)}')

    # Increase fold number
    fold_no = fold_no + 1

model.save('my_model.h5')
# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print(f'> Train Accuracy: {np.mean(train_acc_per_fold)} (+- {np.std(train_acc_per_fold)})')
print(f'> Train Loss: {np.mean(train_loss_per_fold)}')

print('------------------------------------------------------------------------')
print("All DONE:{} ".format(time.time() - start_time))
print(f'Total viruses: {np.count_nonzero(Y_vector)}')
print(f'Total normal Apps: {np.size(Y_vector,0)-np.count_nonzero(Y_vector)}')
print(f'Total APKs: {np.size(Y_vector,0)}')
print()



# Visualize history
# Plot history: Loss
for i in allHistory:
    plt.plot(i.history['loss'],'y')

plt.title('Validation loss history')
plt.ylabel('Loss value')
plt.xlabel('No. epoch')
plt.show()

# Plot history: Accuracy
for i in allHistory:
    plt.plot(i.history['accuracy'],'y')
plt.title('Validation accuracy history')
plt.ylabel('Accuracy value (%)')
plt.xlabel('No. epoch')
plt.show()

test_loss, test_acc = model.evaluate(result_vector[1::10], Y_vector[1::10], verbose=1)
predictions = model.predict(result_vector[:1])
print(predictions)
# Покажем архитектуру модели
model.summary()
